---
title: "XGBoost"
format: 
  html:
    theme: zephyr
    smooth-scroll: true
    toc: true
    toc-location: right
    # self-contained: true
# author: 
#     - name: J.I. Seo
#       affiliations:
#       - Gyeongguk National University
#     - name: J.W. Lee
#       # affiliations:
#       # - University of Missouri
      
number-sections: true
highlight-style: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(width=200)
```

> XGBoost의 장점
 
- 병렬 처리를 사용하여 Gradient Boosting보다 속도가 빠르다.
- Gradient Boosting보다 정확도가 높다.
- 유연성이 좋다.
- 조기 종료가 가능하다.
- 규제를 통해 과적합을 방지할 수 있다.


</br>

> XGBoost의 단점

- Gradient Boosting보다 학습시간이 빠를 뿐 다른 기법에 비하면 느리다.
- 초모수(Hyperparameter)가 많다.
    - 적절하지 않은 초모수 값을 할당하면 오히려 성능이 나빠질 수 있다.

</br>
 
> 실습 자료 : 1912년 4월 15일 타이타닉호 침몰 당시 탑승객들의 정보를 기록한 데이터셋이며, 총 11개의 변수를 포함하고 있다. 이 자료에서 **Target**은 `Survived`이다.

<center>![](./image/그림_titanic.png)</center>

<br />

<center><img src="./image/Titanic_표.png" width="400" height="400"></center>

<br />
 
## 데이터 불러오기


```{r, eval=F}
pacman::p_load("data.table", 
               "tidyverse", 
               "dplyr", "tidyr",
               "ggplot2", "GGally",
               "caret",
               "Matrix",                                                # For sparse.model.matrix
               "xgboost")                                               # For xgb.train

titanic <- fread("../Titanic.csv")                                      # 데이터 불러오기

titanic %>%
  as_tibble
```

```{r, echo=F}
pacman::p_load("data.table", 
               "tidyverse", 
               "dplyr", "tidyr",
               "ggplot2", "GGally",
               "caret",
               "Matrix",                                                # For sparse.model.matrix
               "xgboost")                                               # For xgb.train

titanic <- fread(paste(getwd(), "/DATA/Titanic.csv", sep = "/"))              # 데이터 불러오기

titanic %>%
  as_tibble
```



## 데이터 전처리 I

```{r}
# 1. Convert to Factor
fac.col <- c("Pclass", "Sex",
             # Target
             "Survived")

titanic <- titanic %>% 
  data.frame() %>%                                                      # Data Frame 형태로 변환 
  mutate_at(fac.col, as.factor)                                         # 범주형으로 변환

glimpse(titanic)                                                        # 데이터 구조 확인

# 2. Generate New Variable
titanic <- titanic %>%
  mutate(FamSize = SibSp + Parch)                                       # "FamSize = 형제 및 배우자 수 + 부모님 및 자녀 수"로 가족 수를 의미하는 새로운 변수

glimpse(titanic)                                                        # 데이터 구조 확인

# 3. Select Variables used for Analysis
titanic1 <- titanic %>% 
  dplyr::select(Survived, Pclass, Sex, Age, Fare, FamSize)              # 분석에 사용할 변수 선택

glimpse(titanic1)                                                       # 데이터 구조 확인
```



## 데이터 탐색 

```{r}
ggpairs(titanic1,                                        
        aes(colour = Survived)) +                         # Target의 범주에 따라 색깔을 다르게 표현
  theme_bw()

ggpairs(titanic1,                                     
        aes(colour = Survived, alpha = 0.8)) +            # Target의 범주에 따라 색깔을 다르게 표현
  scale_colour_manual(values = c("#00798c", "#d1495b")) + # 특정 색깔 지정
  scale_fill_manual(values = c("#00798c", "#d1495b")) +   # 특정 색깔 지정
  theme_bw()
```



## 데이터 분할

```{r}
# Partition (Training Dataset : Test Dataset = 7:3)
y      <- titanic1$Survived                           # Target

set.seed(200)
ind    <- createDataPartition(y, p = 0.7, list  =T)   # Index를 이용하여 7:3으로 분할
titanic.trd <- titanic1[ind$Resample1,]               # Training Dataset
titanic.ted <- titanic1[-ind$Resample1,]              # Test Dataset
```



## 데이터 전처리 II

```{r}
# Imputation
titanic.trd.Imp <- titanic.trd %>% 
  mutate(Age = replace_na(Age, mean(Age, na.rm = TRUE)))                 # 평균으로 결측값 대체

titanic.ted.Imp <- titanic.ted %>% 
  mutate(Age = replace_na(Age, mean(titanic.trd$Age, na.rm = TRUE)))     # Training Dataset을 이용하여 결측값 대체

glimpse(titanic.trd.Imp)                                                 # 데이터 구조 확인
glimpse(titanic.ted.Imp)                                                 # 데이터 구조 확인
```



## 모형 훈련

Boosting은 다수의 약한 학습자(간단하면서 성능이 낮은 예측 모형)을 순차적으로 학습하는 앙상블 기법이다. Boosting의 특징은 이전 모형의 오차를 반영하여 다음 모형을 생성하며, 오차를 개선하는 방향으로 학습을 수행한다.

<center>![](./image/boosting.png)</center>

</br>


XGBoost는 Extreme Gradient Boosting의 약어로 Gradient Boosting의 단점을 해결하기 위해 제안되었다. R에서 XGBoost을 수행하기 위해 package `"xgboost"`에서 제공하는 함수 `xgb.train()`를 이용할 수 있으며, 함수의 자세한 옵션은 [여기](https://www.rdocumentation.org/packages/xgboost/versions/1.2.0.1/topics/xgb.train)를 참고한다.   

`Caution!` 함수 `xgb.train()`을 사용하려면 `예측 변수와 Target 모두 수치형`이여야 하며, "xgb.DMatrix"로 변환해야 한다. 이를 위해 다음과 같은 절차를 수행한다.  

1. 범주형 예측 변수를 더미 변수로 변환하기 위해 함수 `sparse.model.matrix()`를 이용한다.  
2. Target을 수치형으로 변환한다.
3. 함수 `xgb.DMatrix()`를 이용하여 "xgb.DMatrix"로 변환한다.

```{r}
# 1. Convert Factor Var. into Dummy Var. 
trainm       <- sparse.model.matrix(Survived ~.-1, # Survived Target으로 제외 
                                    data = titanic.trd.Imp)  

trainm

testm        <- sparse.model.matrix(Survived ~.-1, # Survived Target으로 제외 
                                    data = titanic.ted.Imp) 
testm


# 2. Convert Factor Var. into Numeric Var. for Target
train.y <- as.numeric( as.character( titanic.trd.Imp$Survived ))  

train.y

# 3. Convert to xgb.DMatrix object
train_matrix <- xgb.DMatrix(data = as.matrix(trainm),
                            label = train.y)

train_matrix

test_matrix  <- xgb.DMatrix(data = as.matrix(testm))

test_matrix
```


```{r}
set.seed(100)                                                         # Seed 고정 -> 동일한 결과를 출력하기 위해
titanic.xgb <- xgb.train(data = train_matrix,  
                         watchlist = list(train = train_matrix),      # 모형 구축하는 동안 오차를 계산하기 위해
                         nrounds = 50,                                # nrounds : 반복 수(= 생성하고자 하는 트리 개수) 
                         params = list(objective = "binary:logistic", # 손실함수
                                       eta = 0.01,                    # 학습률
                                       gamma = 0,                     # 분할하기 위해 필요한 최소 손실 감소/ 클수록 분할이 쉽게 일어나지 않음
                                       max_depth = 5,                 # 트리의 최대 깊이
                                       min_child_weight = 1,          # 분할하기 위해 필요한 case의 최소 가중치 합/ 클수록 분할이 쉽게 일어나지 않음
                                       subsample = 1,                 # 트리를 생성할 때 Dataset으로부터 사용할 case 비율
                                       lambda = 1),                   # 규제항
                         early_stopping_rounds = 10)                  # 만약 10번 이후의 반복에서 손실이 개선되지 않으면 조기 종료
```


```{r}
# Training Error Plot
plot(titanic.xgb$evaluation_log$train_logloss, 
     col = "blue",
     type = "l",
     xlab = "iter",
     ylab = "Error")
```

```{r}
# 변수 중요도
importance <- xgb.importance(feature_names = colnames(trainm), model = titanic.xgb)

importance

# 변수 중요도 plot
xgb.plot.importance(importance_matrix = importance) 
```

`Result!` 변수 `Sexmale`이 Target `Survived`을 분류하는 데 있어 중요하다.



## 모형 평가

`Caution!` 모형 평가를 위해 `Test Dataset`에 대한 `예측 class/확률` 이 필요하며, 함수 `predict()`를 이용하여 생성한다. 
```{r}
# "Survived = 1"에 대한 예측 확률 생성
test.xgb.prob <- predict(titanic.xgb,
                         newdata = test_matrix)        # Test Dataset including Only 예측 변수

test.xgb.prob %>%
  as_tibble
```

<br />

### ConfusionMatrix 

```{r}
# 예측 class 생성
cv <- 0.5                                                          # Cutoff Value
test.xgb.class <- as.factor(ifelse(test.xgb.prob > cv, "1", "0"))  # 예측 확률 > cv이면 "Survived = 1" 아니면 "Survived = 0"

test.xgb.class %>%
  as_tibble
```


```{r}
CM   <- caret::confusionMatrix(test.xgb.class, titanic.ted.Imp$Survived, 
                               positive = "1")         # confusionMatrix(예측 class, 실제 class, positive = "관심 class")
CM
```

<br />

### ROC 곡선

```{r}
ac  <- titanic.ted.Imp$Survived                        # Test Dataset의 실제 class 
pp  <- as.numeric(test.xgb.prob)                       # 예측 확률을 수치형으로 변환
```

#### Package "pROC"

```{r}
pacman::p_load("pROC")

xgb.roc  <- roc(ac, pp, plot = T, col = "gray")        # roc(실제 class, 예측 확률)
auc      <- round(auc(xgb.roc), 3)
legend("bottomright", legend = auc, bty = "n")
```

`Caution!` Package `"pROC"`를 통해 출력한 ROC 곡선은 다양한 함수를 이용해서 그래프를 수정할 수 있다.

```{r}
# 함수 plot.roc() 이용
plot.roc(xgb.roc,   
         col="gray",                                   # Line Color
         print.auc = TRUE,                             # AUC 출력 여부
         print.auc.col = "red",                        # AUC 글씨 색깔
         print.thres = TRUE,                           # Cutoff Value 출력 여부
         print.thres.pch = 19,                         # Cutoff Value를 표시하는 도형 모양
         print.thres.col = "red",                      # Cutoff Value를 표시하는 도형의 색깔
         auc.polygon = TRUE,                           # 곡선 아래 면적에 대한 여부
         auc.polygon.col = "gray90")                   # 곡선 아래 면적의 색깔
```


```{r}
# 함수 ggroc() 이용
ggroc(xgb.roc) +
annotate(geom = "text", x = 0.9, y = 1.0,
label = paste("AUC = ", auc),
size = 5,
color="red") +
theme_bw()
```



#### Package "Epi"

```{r}
pacman::p_load("Epi")       
# install_version("etm", version = "1.1", repos = "http://cran.us.r-project.org")

ROC(pp, ac, plot = "ROC")                              # ROC(예측 확률, 실제 class)  
```

#### Package "ROCR"

```{r}
pacman::p_load("ROCR")

xgb.pred <- prediction(pp, ac)                         # prediction(예측 확률, 실제 class) 

xgb.perf <- performance(xgb.pred, "tpr", "fpr")        # performance(, "민감도", "1-특이도")                      
plot(xgb.perf, col = "gray")                           # ROC Curve

perf.auc   <- performance(xgb.pred, "auc")             # AUC
auc        <- attributes(perf.auc)$y.values
legend("bottomright", legend = auc, bty = "n")
```

<br />

### 향상 차트

#### Package "ROCR"

```{r}
xgb.perf <- performance(xgb.pred, "lift", "rpp")       # Lift Chart                      
plot(xgb.perf, main = "lift curve",
     colorize = T,                                     # Coloring according to cutoff 
     lwd = 2) 
```


```{r, eval=F, echo=F, include=FALSE}
#### **2) Package "lift"**

pacman::p_load("lift")

plotLift(test.xgb.prob, titanic.ted.Imp$Survived, cumulative = T, n.buckets = 24)  # plotLift(7-2에서 생성한 예측 확률, 실제 class)
TopDecileLift(test.xgb.prob, titanic.ted.Imp$Survived)		                         # Top 10%의 향상도 출력
```
